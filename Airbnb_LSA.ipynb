{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries we'll use\n",
    "import spacy # fast NLP\n",
    "import pandas as pd # dataframes\n",
    "import langid # language identification (i.e. what language is this?)\n",
    "from nltk.classify.textcat import TextCat # language identification from NLTK\n",
    "from matplotlib.pyplot import plot # not as good as ggplot in R :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AirData=pd.read_csv(\"reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AirData[\"comments\"]=AirData[\"comments\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tagged languages (estimated):\n",
      "49\n",
      "Percent of data in English (estimated):\n",
      "95.1476374041251\n"
     ]
    }
   ],
   "source": [
    "ids_langid = AirData[\"comments\"].apply(langid.classify)\n",
    "\n",
    "# get just the language label\n",
    "langs = ids_langid.apply(lambda tuple: tuple[0])\n",
    "\n",
    "# how many unique language labels were applied?\n",
    "print(\"Number of tagged languages (estimated):\")\n",
    "print(len(langs.unique()))\n",
    "\n",
    "# percent of the total dataset in English\n",
    "print(\"Percent of data in English (estimated):\")\n",
    "print((sum(langs==\"en\")/len(langs))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "English_comment=AirData[\"comments\"][langs=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADITYA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95768, 10000)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf vectorizer of scikit learn\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words,max_features=10000, max_df = 0.5, use_idf = True, ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(English_comment)\n",
    "print(X.shape) # check shape of the document-term matrix\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<95768x12472 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1618138 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0: \n",
      "light clean\n",
      "stay apartment\n",
      "clean hosts\n",
      "place great value\n",
      "pictured\n",
      "day exploring\n",
      "wanted stay\n",
      " \n",
      "Concept 1: \n",
      "wanted stay\n",
      "lovely friendly\n",
      "place great value\n",
      "clean hosts\n",
      "ride bristol\n",
      "throughout stay\n",
      "lots nice touches\n",
      " \n",
      "Concept 2: \n",
      "wanted stay\n",
      "lovely friendly\n",
      "place great value\n",
      "pictured\n",
      "day exploring\n",
      "large clean\n",
      "ride bristol\n",
      " \n",
      "Concept 3: \n",
      "stay apartment\n",
      "ride bristol\n",
      "clean hosts\n",
      "large clean\n",
      "day exploring\n",
      "throughout stay\n",
      "cut\n",
      " \n",
      "Concept 4: \n",
      "hostess\n",
      "clean hosts\n",
      "pictured\n",
      "bed big\n",
      "space quiet\n",
      "communication easy\n",
      "graffiti\n",
      " \n",
      "Concept 5: \n",
      "place great value\n",
      "stay apartment\n",
      "hostess\n",
      "tiny\n",
      "lots nice touches\n",
      "lockbox\n",
      "decoration\n",
      " \n",
      "Concept 6: \n",
      "clean hosts\n",
      "lovely friendly\n",
      "lots nice touches\n",
      "light clean\n",
      "cut\n",
      "communication easy\n",
      "bus town\n",
      " \n",
      "Concept 7: \n",
      "throughout stay\n",
      "lovely friendly\n",
      "ride bristol\n",
      "light clean\n",
      "great place lovely\n",
      "lots nice touches\n",
      "hostess\n",
      " \n",
      "Concept 8: \n",
      "wanted stay\n",
      "hostess\n",
      "clean hosts\n",
      "light clean\n",
      "buzzing\n",
      "discreet\n",
      "lots nice touches\n",
      " \n",
      "Concept 9: \n",
      "large clean\n",
      "place great value\n",
      "ride bristol\n",
      "wanted stay\n",
      "clean hosts\n",
      "communication easy\n",
      "cut\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "U, Sigma, VT = randomized_svd(doc_term_matrix, n_components=10, n_iter=100,\n",
    "                              random_state=122)\n",
    "#printing the concepts\n",
    "for i, comp in enumerate(VT):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "        print(\"Concept \"+str(i)+\": \")\n",
    "        for t in sorted_terms:\n",
    "            print(t[0])\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
